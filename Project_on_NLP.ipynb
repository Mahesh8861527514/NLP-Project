{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NATURAL LANGUAGE PROCESSING\n",
        "\n",
        "1.Tokenization\n",
        "2.Stemming\n",
        "3.Lamitization\n",
        "\n",
        "NLP uses its library called\n",
        "'NLTK\" Natural Language Toolkit"
      ],
      "metadata": {
        "id": "UcZeHwdO_wNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnU5xnop_rt9"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"popular\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOkSjhSsSBfF",
        "outputId": "9a15f36c-aac9-46ad-8d5c-4f9cf982ccab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"I am talking about the events of a thousand years ago for a reason.\n",
        "I am witnessing another opportunity before our country,\n",
        "a time when we are living in such a period,\n",
        "when we have entered such an era.\n",
        "It is our good fortune that either we are living in youth or we have taken birth in the lap of Mother India in the first year of ‘Amrit Kaal’.\n",
        " And mark my words, my dear family members, the actions we take, the steps we take, the sacrifices we make,\n",
        " the penance we undertake in this era will define our legacy.\"\"\""
      ],
      "metadata": {
        "id": "7B8pO5vjSLYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "9B4UbLYr8rK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence level tokenization\n",
        "sentences=nltk.sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "DT2ss7rB8xDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBme4_dH-NlZ",
        "outputId": "b83f97ca-5f2a-4576-dc4c-f22e57282323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am talking about the events of a thousand years ago for a reason.', 'I am witnessing another opportunity before our country,\\na time when we are living in such a period,\\nwhen we have entered such an era.', 'It is our good fortune that either we are living in youth or we have taken birth in the lap of Mother India in the first year of ‘Amrit Kaal’.', 'And mark my words, my dear family members, the actions we take, the steps we take, the sacrifices we make,\\n the penance we undertake in this era will define our legacy.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word level tokenization\n",
        "words=nltk.word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "2HMt93_D-iBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZPDfWRK73C",
        "outputId": "310d6e5b-0070-41a3-de4e-325fb5c32224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'talking', 'about', 'the', 'events', 'of', 'a', 'thousand', 'years', 'ago', 'for', 'a', 'reason', '.', 'I', 'am', 'witnessing', 'another', 'opportunity', 'before', 'our', 'country', ',', 'a', 'time', 'when', 'we', 'are', 'living', 'in', 'such', 'a', 'period', ',', 'when', 'we', 'have', 'entered', 'such', 'an', 'era', '.', 'It', 'is', 'our', 'good', 'fortune', 'that', 'either', 'we', 'are', 'living', 'in', 'youth', 'or', 'we', 'have', 'taken', 'birth', 'in', 'the', 'lap', 'of', 'Mother', 'India', 'in', 'the', 'first', 'year', 'of', '‘', 'Amrit', 'Kaal', '’', '.', 'And', 'mark', 'my', 'words', ',', 'my', 'dear', 'family', 'members', ',', 'the', 'actions', 'we', 'take', ',', 'the', 'steps', 'we', 'take', ',', 'the', 'sacrifices', 'we', 'make', ',', 'the', 'penance', 'we', 'undertake', 'in', 'this', 'era', 'will', 'define', 'our', 'legacy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "-OvZpm6l1S2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words(\"english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Pwq85i1ehJ",
        "outputId": "d5fb0184-0578-4c87-8157-0165130f87f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "U0YfBLds0lHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#word level stemming\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "NXE7RrdFK-Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "nGMSwCxP52xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(sentences)): #No. of words in the sentence\n",
        "  words=nltk.word_tokenize(sentences[i])#words in the sentence will be tokenoized\n",
        "  words=[stemming.stem(word) for word in words if word not in set(stopwords.words('english'))]#stopwords will be eliminated\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "KOYeauV957D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5MVjHOK6dhS",
        "outputId": "d7bd4bca-922a-429c-cc99-3ba1e98ecf5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['talk event thousand year ago reason .', 'wit anoth opportun countri , time live period , enter era .', 'good fortun either live youth taken birth lap mother india first year ‘ amrit kaal ’ .', 'mark word , dear famili member , action take , step take , sacrif make , penanc undertak era defin legaci .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "rFObko1E7tkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ruGtK3l5A98I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(sentences)): #No. of words in the sentence\n",
        "  words=nltk.word_tokenize(sentences[i])#words in the sentence will be tokenoized\n",
        "  words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]#stopwords will be eliminated\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "BC7c4-GtBE76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMrOIMCyBVpA",
        "outputId": "2a48a2c6-e876-4a35-f024-ef06b8b5ee4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['talk event thousand year ago reason .', 'wit anoth opportun countri , time live period , enter era .', 'good fortun either live youth taken birth lap mother india first year ‘ amrit kaal ’ .', 'mark word , dear famili member , action take , step take , sacrif make , penanc undertak era defin legaci .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0R7C1oZmBlYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}